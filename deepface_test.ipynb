{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Library Import"
      ],
      "metadata": {
        "id": "CNXBblJSp8hf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dlFSOnD8-7ml",
        "outputId": "93e2d0b5-5af7-4c95-fde7-a7b4b8ae1f24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting deepface\n",
            "  Downloading deepface-0.0.93-py3-none-any.whl.metadata (30 kB)\n",
            "Requirement already satisfied: tf-keras in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: requests>=2.27.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.32.3)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (1.26.4)\n",
            "Requirement already satisfied: pandas>=0.23.4 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.2.2)\n",
            "Requirement already satisfied: gdown>=3.10.1 in /usr/local/lib/python3.11/dist-packages (from deepface) (5.2.0)\n",
            "Requirement already satisfied: tqdm>=4.30.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (4.67.1)\n",
            "Requirement already satisfied: Pillow>=5.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (11.1.0)\n",
            "Requirement already satisfied: opencv-python>=4.5.5.64 in /usr/local/lib/python3.11/dist-packages (from deepface) (4.11.0.86)\n",
            "Requirement already satisfied: tensorflow>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (2.18.0)\n",
            "Requirement already satisfied: keras>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.8.0)\n",
            "Requirement already satisfied: Flask>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from deepface) (3.1.0)\n",
            "Collecting flask-cors>=4.0.1 (from deepface)\n",
            "  Downloading flask_cors-5.0.1-py3-none-any.whl.metadata (961 bytes)\n",
            "Collecting mtcnn>=0.1.0 (from deepface)\n",
            "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting retina-face>=0.0.1 (from deepface)\n",
            "  Downloading retina_face-0.0.17-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting fire>=0.4.0 (from deepface)\n",
            "  Downloading fire-0.7.0.tar.gz (87 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gunicorn>=20.1.0 (from deepface)\n",
            "  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.11/dist-packages (from fire>=0.4.0->deepface) (2.5.0)\n",
            "Requirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.3)\n",
            "Requirement already satisfied: Jinja2>=3.1.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (3.1.5)\n",
            "Requirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (2.2.0)\n",
            "Requirement already satisfied: click>=8.1.3 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (8.1.8)\n",
            "Requirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask>=1.1.2->deepface) (1.9.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (4.13.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from gdown>=3.10.1->deepface) (3.17.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (1.4.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.0.8)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (3.12.1)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.14.0)\n",
            "Requirement already satisfied: ml-dtypes in /usr/local/lib/python3.11/dist-packages (from keras>=2.2.0->deepface) (0.4.1)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from mtcnn>=0.1.0->deepface) (1.4.2)\n",
            "Collecting lz4>=4.3.3 (from mtcnn>=0.1.0->deepface)\n",
            "  Downloading lz4-4.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.23.4->deepface) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.1->deepface) (2025.1.31)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (4.25.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (75.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (1.70.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow>=1.9.0->deepface) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow>=1.9.0->deepface) (0.45.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2>=3.1.2->Flask>=1.1.2->deepface) (3.0.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow>=1.9.0->deepface) (0.7.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->gdown>=3.10.1->deepface) (2.6)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /usr/local/lib/python3.11/dist-packages (from requests[socks]->gdown>=3.10.1->deepface) (1.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=2.2.0->deepface) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=2.2.0->deepface) (0.1.2)\n",
            "Downloading deepface-0.0.93-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.6/108.6 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flask_cors-5.0.1-py3-none-any.whl (11 kB)\n",
            "Downloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading retina_face-0.0.17-py3-none-any.whl (25 kB)\n",
            "Downloading lz4-4.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: fire\n",
            "  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fire: filename=fire-0.7.0-py3-none-any.whl size=114249 sha256=0369d172881dc037500d62ba990875c261364656ab033879e8cd46116f99a02a\n",
            "  Stored in directory: /root/.cache/pip/wheels/46/54/24/1624fd5b8674eb1188623f7e8e17cdf7c0f6c24b609dfb8a89\n",
            "Successfully built fire\n",
            "Installing collected packages: lz4, gunicorn, fire, mtcnn, flask-cors, retina-face, deepface\n",
            "Successfully installed deepface-0.0.93 fire-0.7.0 flask-cors-5.0.1 gunicorn-23.0.0 lz4-4.4.3 mtcnn-1.0.0 retina-face-0.0.17\n"
          ]
        }
      ],
      "source": [
        "!pip install deepface tf-keras matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Library Install"
      ],
      "metadata": {
        "id": "dfzqRd8lqicB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepface import DeepFace\n",
        "from retinaface import RetinaFace"
      ],
      "metadata": {
        "id": "jSR6O075WeWX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c1daa97-852a-4517-9154-af3b387758e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25-02-28 03:44:39 - Directory /root/.deepface has been created\n",
            "25-02-28 03:44:39 - Directory /root/.deepface/weights has been created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image, ImageDraw\n",
        "import glob"
      ],
      "metadata": {
        "id": "0wKrK0FqMP2V"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json"
      ],
      "metadata": {
        "id": "k87vl5xNuHFH"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use Google Colab"
      ],
      "metadata": {
        "id": "MnhqkD_FrDzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')\n",
        "ROOT_DIR = \"/content/gdrive/MyDrive/Colab Notebooks/FR_Project/\""
      ],
      "metadata": {
        "id": "mgAvVTNWp7zM",
        "outputId": "f30c0c56-f30e-4ab5-aff0-cb168f3142fc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Logic"
      ],
      "metadata": {
        "id": "sHiZe3r0rINl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## File structure"
      ],
      "metadata": {
        "id": "gghA3MP61i12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import shutil\n",
        "\n",
        "\n",
        "def mkdir(p):\n",
        "  Path(p).mkdir(parents=True, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "oHFPxbeo2dey"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "SOURCE_DIR = f\"{ROOT_DIR}Raw_Image_Source\"\n",
        "PROCESSED_IMAGE_TXT = f\"Processed.txt\"\n",
        "\n",
        "score_index_list = {}\n",
        "non_face_list = []"
      ],
      "metadata": {
        "id": "0I6iB-f_1naS"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DRIVE_OUTPUT = False\n",
        "\n",
        "if DRIVE_OUTPUT:\n",
        "  FACES_INDEX = f\"{ROOT_DIR}Faces_Index\"\n",
        "  FACES_DIR = f\"{ROOT_DIR}Faces\"\n",
        "  SCORE_INDEX_LIST_PATH = f\"{FACES_INDEX}/score_index_list.json\"\n",
        "else:\n",
        "  FACES_INDEX = f\"Faces_Index\"\n",
        "  FACES_DIR = f\"Faces\"\n",
        "  SCORE_INDEX_LIST_PATH = f\"{FACES_INDEX}/score_index_list.json\"\n",
        "\n",
        "NON_FACES_LIST_PATH = f\"{ROOT_DIR}Non_Faces.txt\"\n",
        "# TEMP_DIR = \"Temp\"\n",
        "mkdir(FACES_INDEX)\n",
        "mkdir(FACES_DIR)\n",
        "# mkdir(TEMP_DIR)\n",
        "\n",
        "def write_processed(img_path):\n",
        "  with open(PROCESSED_IMAGE_TXT, \"a+\") as txt_file:\n",
        "    txt_file.writelines(img_path + \"\\n\")\n"
      ],
      "metadata": {
        "id": "DPYzr9EK2tNi"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Init JSON score index list"
      ],
      "metadata": {
        "id": "XXLVXkJIuRYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def init_score_index():\n",
        "  if not Path(SCORE_INDEX_LIST_PATH).is_file():\n",
        "    score_index_list = {}\n",
        "    with open(SCORE_INDEX_LIST_PATH, 'w') as f:\n",
        "          json.dump({}, f)\n",
        "\n",
        "  with open(SCORE_INDEX_LIST_PATH, 'r+') as f:\n",
        "      score_index_list = json.load(f)"
      ],
      "metadata": {
        "id": "WgUiAeA3uOAz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def init_dummy():\n",
        "  img = Image.new('RGB',(480,640),\"rgb(255,255,255)\")\n",
        "  img.save(f\"{FACES_INDEX}/dummy.jpg\")"
      ],
      "metadata": {
        "id": "DVJFDT4hFy7s"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_nonface_list():\n",
        "  if not Path(NON_FACES_LIST_PATH).is_file():\n",
        "    non_face_list = []\n",
        "  else:\n",
        "    with open(NON_FACES_LIST_PATH, \"r\") as f:\n",
        "      non_face_list = [l.replace(\"\\n\", \"\") for l in f.readlines()]\n",
        "\n",
        "\n",
        "def write_to_nonface_list(img_path):\n",
        "  global non_face_list\n",
        "  if not Path(NON_FACES_LIST_PATH).is_file():\n",
        "    non_face_list = []\n",
        "\n",
        "  with open(NON_FACES_LIST_PATH, \"a+\") as f:\n",
        "    f.writelines(img_path + \"\\n\")\n",
        "\n",
        "  non_face_list.append(img_path)"
      ],
      "metadata": {
        "id": "eyDsoG9BWL9v"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Function:** Image view with PIL\n",
        "\n",
        "drawImageWithPlot( image_Path, param, save_crop=False, *, show_img=False )\n",
        "\n",
        "---\n",
        "\n",
        "> param: {\n",
        "  \"face_area_1\": {\n",
        "    facial_area: [x, y, w, h],\n",
        "    ...\n",
        "  },\n",
        "  \"face_area_2\": {\n",
        "    facial_area: [x, y, w, h],\n",
        "    ...\n",
        "  }, ...\n",
        "}"
      ],
      "metadata": {
        "id": "T7Mfzj_6rZd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plt_show_img(imgs: list) -> None:\n",
        "  imgs_count = len(imgs)\n",
        "  for i in range(imgs_count):\n",
        "    ax = plt.subplot(1, imgs_count, i+1)\n",
        "    ax.imshow(imgs[i])\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "  plt.tight_layout(h_pad=3)\n",
        "  plt.show()\n",
        "\n",
        "def drawImageWithPlot(img_path, param, save_crop=False, *, show_img=False):\n",
        "  im = Image.open(img_path)\n",
        "  crop_array = []\n",
        "  crop_len = len(crop_array)\n",
        "  crop_count = 0\n",
        "\n",
        "  draw_img_grid = []\n",
        "\n",
        "  for key in param:\n",
        "    rect = param[key]['facial_area']\n",
        "    x, y, w, h = rect\n",
        "\n",
        "    if save_crop > 0:\n",
        "      im_crop = im.crop((x-100, y-100, w+100, h+100))\n",
        "      save_path = crop_array[crop_count] if crop_count < crop_len else f\"{FACES_DIR}/saved_crop_{crop_count}.jpg\"\n",
        "      im_crop.save(save_path)\n",
        "      crop_count = crop_count + 1\n",
        "\n",
        "    border_width = int(min(w-x, h-y) / 15)\n",
        "    border_width = max(border_width, 10)\n",
        "    border_width = min(border_width, 20)\n",
        "\n",
        "    draw_img_grid.append((x, y, w, h, border_width))\n",
        "    # draw = ImageDraw.Draw(im)\n",
        "    # draw.rectangle((x, y, w, h), outline=\"red\", width=border_width)\n",
        "\n",
        "  if show_img:\n",
        "    draw = ImageDraw.Draw(im)\n",
        "    for x, y, w, h, border_width in draw_img_grid:\n",
        "      draw.rectangle((x, y, w, h), outline=\"red\", width=border_width)\n",
        "    plt_show_img([im])"
      ],
      "metadata": {
        "id": "ewFFw0s8VoEm"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Function: Save Crop Image as Face_index"
      ],
      "metadata": {
        "id": "mua2H3ktLkDl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model List\n",
        "# models_name = [\"VGG-Face\", \"Facenet\", \"Facenet512\", \"OpenFace\",\n",
        "#         \"DeepID\", \"ArcFace\", \"SFace\"]\n",
        "models_name = [\"Facenet512\", \"OpenFace\", \"DeepID\", \"SFace\"]"
      ],
      "metadata": {
        "id": "lVmn46FjvN_7"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_next_img_full_path():\n",
        "  i = 1\n",
        "  while True:\n",
        "    if f\"{FACES_INDEX}/person{i}.jpg\" not in score_index_list:\n",
        "      return f\"{FACES_INDEX}/person{i}.jpg\"\n",
        "    i = i + 1"
      ],
      "metadata": {
        "id": "YfvS2Y0A0eSo"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_face(face_index, img_path, crop_img = ''):\n",
        "  face_path = f'{FACES_DIR}/{face_index}'\n",
        "  mkdir(face_path)\n",
        "  copied_path = shutil.copy(img_path, face_path)\n",
        "\n",
        "  if crop_img:\n",
        "    shutil.copy(crop_img, f\"{copied_path[:-4]}_Crop.jpg\")\n",
        ""
      ],
      "metadata": {
        "id": "HbFQNZSPW2Sl"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# True: replace\n",
        "# False: index no change\n",
        "def check_new_index_file(img_name, score) -> bool:\n",
        "  if img_name not in score_index_list:\n",
        "    return True\n",
        "  curr_score = score_index_list[img_name]\n",
        "  return curr_score < score\n",
        "\n",
        "def check_face_index(img_path, param, *, slient = True):\n",
        "  def print_log(*arg):\n",
        "    if not slient: print(arg)\n",
        "\n",
        "  im = Image.open(img_path)\n",
        "  found_person = [f'{FACES_INDEX}/dummy.jpg'] # two person will not appear in same image\n",
        "  image_valid = False\n",
        "\n",
        "  for key in param:\n",
        "    score = param[key]['score']\n",
        "    rect = param[key]['facial_area']\n",
        "    x, y, w, h = rect\n",
        "    total_width = w - x\n",
        "    total_height = h - y\n",
        "\n",
        "    im_crop = im.crop((x - total_width*0.3, y - total_height*0.4, w + total_width*0.3, h + total_height*0.4))\n",
        "\n",
        "    # filter too small images\n",
        "    if w - x <= 64 or h - y <= 64:\n",
        "      print_log(f\"image too small: {rect}\")\n",
        "      continue\n",
        "\n",
        "    # filter score too low?\n",
        "    if score < 0.96:\n",
        "      print_log(f\"score too low: {score}\")\n",
        "      continue\n",
        "\n",
        "    save_path = f\"Crop_Image_Temp.jpg\"\n",
        "    im_crop.save(save_path)\n",
        "\n",
        "    match_res = []\n",
        "    match_model = []\n",
        "    result = DeepFace.find(img_path=save_path, db_path=FACES_INDEX, enforce_detection=False, silent=True)\n",
        "    for res in result:\n",
        "      if res.empty: continue\n",
        "      r = res[\"identity\"][0]\n",
        "      if r not in match_res:\n",
        "        match_res.append(r)\n",
        "      match_model.append(x)\n",
        "\n",
        "    print_log(f\"Image Score: {score}\")\n",
        "    print_log(f\"Matched: {match_res}\")\n",
        "    print_log(f\"Reported by: {match_model}\")\n",
        "    found_img_full_path = ''\n",
        "\n",
        "    for r in match_res:\n",
        "      if r in found_person: continue\n",
        "      obj = DeepFace.verify(img1_path = r, img2_path = save_path,\n",
        "          model_name=\"Facenet512\" ,detector_backend = 'retinaface',\n",
        "          enforce_detection=False, threshold=0.6)\n",
        "      print_log(obj)\n",
        "      if obj[\"verified\"]:\n",
        "        found_img_full_path = r\n",
        "        break\n",
        "    if not found_img_full_path:\n",
        "      found_img_full_path = gen_next_img_full_path()\n",
        "    face_key = found_img_full_path.split('/')[-1].replace(\".jpg\", '')\n",
        "\n",
        "    if check_new_index_file(found_img_full_path, score):\n",
        "      if found_img_full_path in score_index_list:\n",
        "        print_log(f\"Replacing {found_img_full_path} from {score_index_list[found_img_full_path]} -> {score}\")\n",
        "      else:\n",
        "        print_log(f\"New Person: {found_img_full_path} with score {score}\")\n",
        "\n",
        "      shutil.copy(save_path, f\"{found_img_full_path}\")\n",
        "      score_index_list[found_img_full_path] = score\n",
        "      with open(SCORE_INDEX_LIST_PATH, 'w') as f:\n",
        "        json.dump(score_index_list, f)\n",
        "\n",
        "      Path.unlink(f'{FACES_INDEX}/dummy.jpg', True)\n",
        "\n",
        "    save_face(face_key, img_path, save_path)\n",
        "    Path.unlink(save_path, True)\n",
        "    found_person.append(found_img_full_path)\n",
        "    image_valid = True\n",
        "    print_log()\n",
        "\n",
        "  return image_valid"
      ],
      "metadata": {
        "id": "cYAEyAvDLi0Q"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Analyze"
      ],
      "metadata": {
        "id": "HZ1P04TIuGNe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_file_structure():\n",
        "  global score_index_list\n",
        "\n",
        "  shutil.rmtree(FACES_DIR, ignore_errors=True)\n",
        "  score_index_list = {}\n",
        "  index_list = glob.glob(f\"{FACES_INDEX}/*.jpg\")\n",
        "  for jpg in index_list:\n",
        "    Path.unlink(jpg)\n",
        "  Path.unlink(SCORE_INDEX_LIST_PATH, missing_ok=True)\n",
        "  Path.unlink(PROCESSED_IMAGE_TXT, missing_ok=True)\n",
        "\n",
        "  mkdir(FACES_DIR)\n",
        "\n",
        "reset_file_structure()"
      ],
      "metadata": {
        "id": "YLvFIcwGfQoy"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "init_score_index()\n",
        "init_dummy()\n",
        "image_list = glob.glob(f\"{SOURCE_DIR}/*.jpg\")\n",
        "\n",
        "if Path(NON_FACES_LIST_PATH).is_file():\n",
        "  with open(NON_FACES_LIST_PATH, \"r\") as txt_file:\n",
        "    lines = txt_file.readlines()\n",
        "\n",
        "  for x in lines:\n",
        "    image_list.remove(x.replace(\"\\n\", ''))\n",
        "\n",
        "\n",
        "Path(PROCESSED_IMAGE_TXT).touch(exist_ok=True)\n",
        "with open(PROCESSED_IMAGE_TXT, \"r\") as txt_file:\n",
        "  lines = txt_file.readlines()\n",
        "\n",
        "for x in lines:\n",
        "  try:\n",
        "    image_list.remove(x.replace(\"\\n\", ''))\n",
        "  except:\n",
        "    pass"
      ],
      "metadata": {
        "id": "4WpDHGKdP5y2"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "progress_count = 0\n",
        "total = len(image_list)\n",
        "for img_path in image_list:\n",
        "  resp = RetinaFace.detect_faces(img_path)\n",
        "\n",
        "  write_processed(img_path)\n",
        "  image_list.remove(img_path)\n",
        "  # drawImageWithPlot(img_path, resp, True, False)\n",
        "  if resp:\n",
        "    # if check_face_index(img_path, resp):\n",
        "    #   break\n",
        "    check_face_index(img_path, resp)\n",
        "  else:\n",
        "    write_to_nonface_list(img_path)\n",
        "\n",
        "  progress_count = progress_count + 1\n",
        "  print(f\"Progress: {progress_count} / {total}\")"
      ],
      "metadata": {
        "id": "oEtTIRM_432n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4c01a7bd-35dc-44b6-a9f6-1743cb4fe017"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Progress: 1 / 8831\n",
            "Progress: 2 / 8831\n",
            "Progress: 3 / 8831\n",
            "Progress: 4 / 8831\n",
            "Progress: 5 / 8831\n",
            "Progress: 6 / 8831\n",
            "Progress: 7 / 8831\n",
            "Progress: 8 / 8831\n",
            "Progress: 9 / 8831\n",
            "Progress: 10 / 8831\n",
            "Progress: 11 / 8831\n",
            "{'verified': False, 'distance': 0.5816141615666983, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 54, 'y': 81, 'w': 137, 'h': 249, 'left_eye': (79, 183), 'right_eye': (75, 174)}, 'img2': {'x': 97, 'y': 124, 'w': 600, 'h': 635, 'left_eye': (399, 424), 'right_eye': (209, 488)}}, 'time': 0.9}\n",
            "Progress: 12 / 8831\n",
            "{'verified': False, 'distance': 0.47377979553899763, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 97, 'y': 124, 'w': 600, 'h': 635, 'left_eye': (399, 424), 'right_eye': (209, 488)}, 'img2': {'x': 24, 'y': 29, 'w': 144, 'h': 161, 'left_eye': (111, 90), 'right_eye': (47, 95)}}, 'time': 1.88}\n",
            "Progress: 13 / 8831\n",
            "{'verified': False, 'distance': 0.4987299800278633, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 54, 'y': 80, 'w': 165, 'h': 201, 'left_eye': (175, 165), 'right_eye': (98, 151)}, 'img2': {'x': 39, 'y': 62, 'w': 154, 'h': 190, 'left_eye': (159, 141), 'right_eye': (87, 146)}}, 'time': 0.87}\n",
            "{'verified': True, 'distance': 0.15137316328488992, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 69, 'y': 109, 'w': 208, 'h': 254, 'left_eye': (229, 216), 'right_eye': (130, 206)}, 'img2': {'x': 40, 'y': 65, 'w': 126, 'h': 158, 'left_eye': (124, 115), 'right_eye': (65, 133)}}, 'time': 0.88}\n",
            "{'verified': False, 'distance': 0.37380231875481995, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 54, 'y': 81, 'w': 137, 'h': 249, 'left_eye': (79, 183), 'right_eye': (75, 174)}, 'img2': {'x': 0, 'y': 1, 'w': 71, 'h': 96, 'left_eye': (21, 37), 'right_eye': (11, 36)}}, 'time': 0.88}\n",
            "{'verified': True, 'distance': 0.06985394539308754, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 54, 'y': 80, 'w': 165, 'h': 201, 'left_eye': (175, 165), 'right_eye': (98, 151)}, 'img2': {'x': 30, 'y': 56, 'w': 106, 'h': 129, 'left_eye': (109, 102), 'right_eye': (58, 110)}}, 'time': 0.87}\n",
            "{'verified': True, 'distance': 0.2127078292491691, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 42, 'y': 72, 'w': 143, 'h': 158, 'left_eye': (131, 131), 'right_eye': (68, 132)}, 'img2': {'x': 31, 'y': 48, 'w': 89, 'h': 112, 'left_eye': (106, 93), 'right_eye': (66, 84)}}, 'time': 0.61}\n",
            "{'verified': True, 'distance': 0.11091399045776329, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 26, 'y': 42, 'w': 94, 'h': 126, 'left_eye': (91, 92), 'right_eye': (48, 97)}, 'img2': {'x': 23, 'y': 38, 'w': 67, 'h': 88, 'left_eye': (74, 73), 'right_eye': (41, 69)}}, 'time': 0.59}\n",
            "{'verified': True, 'distance': 0.21336023739735965, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 74, 'y': 113, 'w': 222, 'h': 257, 'left_eye': (219, 190), 'right_eye': (125, 241)}, 'img2': {'x': 44, 'y': 54, 'w': 117, 'h': 129, 'left_eye': (139, 99), 'right_eye': (85, 104)}}, 'time': 0.92}\n",
            "{'verified': False, 'distance': 0.3071242408223248, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 54, 'y': 81, 'w': 137, 'h': 249, 'left_eye': (79, 183), 'right_eye': (75, 174)}, 'img2': {'x': 21, 'y': 27, 'w': 77, 'h': 121, 'left_eye': (43, 74), 'right_eye': (41, 68)}}, 'time': 0.94}\n",
            "Progress: 14 / 8831\n",
            "{'verified': True, 'distance': 0.04884905224630165, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 39, 'y': 62, 'w': 154, 'h': 190, 'left_eye': (159, 141), 'right_eye': (87, 146)}, 'img2': {'x': 45, 'y': 61, 'w': 148, 'h': 202, 'left_eye': (156, 143), 'right_eye': (89, 142)}}, 'time': 1.18}\n",
            "{'verified': True, 'distance': 0.07347788310240722, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 54, 'y': 80, 'w': 165, 'h': 201, 'left_eye': (175, 165), 'right_eye': (98, 151)}, 'img2': {'x': 31, 'y': 52, 'w': 104, 'h': 127, 'left_eye': (106, 98), 'right_eye': (57, 105)}}, 'time': 0.67}\n",
            "{'verified': True, 'distance': 0.09870231569790922, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 69, 'y': 109, 'w': 208, 'h': 254, 'left_eye': (229, 216), 'right_eye': (130, 206)}, 'img2': {'x': 39, 'y': 57, 'w': 127, 'h': 173, 'left_eye': (127, 131), 'right_eye': (69, 132)}}, 'time': 0.96}\n",
            "{'verified': True, 'distance': 0.18272798154621428, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 29, 'y': 55, 'w': 113, 'h': 169, 'left_eye': (122, 120), 'right_eye': (74, 121)}, 'img2': {'x': 22, 'y': 42, 'w': 95, 'h': 124, 'left_eye': (76, 90), 'right_eye': (37, 94)}}, 'time': 1.03}\n",
            "{'verified': True, 'distance': 0.23693130388654704, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 42, 'y': 72, 'w': 143, 'h': 158, 'left_eye': (131, 131), 'right_eye': (68, 132)}, 'img2': {'x': 33, 'y': 43, 'w': 88, 'h': 111, 'left_eye': (107, 89), 'right_eye': (68, 80)}}, 'time': 0.59}\n",
            "{'verified': True, 'distance': 0.10647143934398329, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 26, 'y': 42, 'w': 94, 'h': 126, 'left_eye': (91, 92), 'right_eye': (48, 97)}, 'img2': {'x': 21, 'y': 29, 'w': 72, 'h': 96, 'left_eye': (74, 70), 'right_eye': (41, 67)}}, 'time': 0.58}\n",
            "Progress: 15 / 8831\n",
            "{'verified': True, 'distance': 0.06277434194865228, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 54, 'y': 80, 'w': 165, 'h': 201, 'left_eye': (175, 165), 'right_eye': (98, 151)}, 'img2': {'x': 66, 'y': 111, 'w': 216, 'h': 256, 'left_eye': (206, 220), 'right_eye': (110, 199)}}, 'time': 0.72}\n",
            "{'verified': True, 'distance': 0.2675554130408856, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 42, 'y': 72, 'w': 143, 'h': 158, 'left_eye': (131, 131), 'right_eye': (68, 132)}, 'img2': {'x': 61, 'y': 93, 'w': 170, 'h': 210, 'left_eye': (196, 174), 'right_eye': (115, 173)}}, 'time': 1.11}\n",
            "{'verified': True, 'distance': 0.18884016112692115, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 22, 'y': 42, 'w': 95, 'h': 124, 'left_eye': (76, 90), 'right_eye': (37, 94)}, 'img2': {'x': 88, 'y': 147, 'w': 336, 'h': 425, 'left_eye': (255, 301), 'right_eye': (127, 324)}}, 'time': 0.78}\n",
            "{'verified': True, 'distance': 0.1691794595399242, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 69, 'y': 109, 'w': 208, 'h': 254, 'left_eye': (229, 216), 'right_eye': (130, 206)}, 'img2': {'x': 32, 'y': 50, 'w': 101, 'h': 142, 'left_eye': (103, 108), 'right_eye': (57, 111)}}, 'time': 1.68}\n",
            "{'verified': True, 'distance': 0.20824576119836558, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 45, 'y': 61, 'w': 148, 'h': 202, 'left_eye': (156, 143), 'right_eye': (89, 142)}, 'img2': {'x': 33, 'y': 57, 'w': 103, 'h': 130, 'left_eye': (108, 100), 'right_eye': (59, 111)}}, 'time': 1.47}\n",
            "{'verified': True, 'distance': 0.1026235453483072, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 26, 'y': 42, 'w': 94, 'h': 126, 'left_eye': (91, 92), 'right_eye': (48, 97)}, 'img2': {'x': 34, 'y': 52, 'w': 117, 'h': 152, 'left_eye': (122, 114), 'right_eye': (65, 113)}}, 'time': 1.21}\n",
            "{'verified': True, 'distance': 0.2371536274811017, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 45, 'y': 51, 'w': 119, 'h': 125, 'left_eye': (142, 97), 'right_eye': (90, 100)}, 'img2': {'x': 42, 'y': 78, 'w': 135, 'h': 160, 'left_eye': (148, 143), 'right_eye': (86, 135)}}, 'time': 1.11}\n",
            "Progress: 16 / 8831\n",
            "Progress: 17 / 8831\n",
            "Progress: 18 / 8831\n",
            "Progress: 19 / 8831\n",
            "Progress: 20 / 8831\n",
            "Progress: 21 / 8831\n",
            "Progress: 22 / 8831\n",
            "Progress: 23 / 8831\n",
            "Progress: 24 / 8831\n",
            "Progress: 25 / 8831\n",
            "Progress: 26 / 8831\n",
            "Progress: 27 / 8831\n",
            "Progress: 28 / 8831\n",
            "Progress: 29 / 8831\n",
            "Progress: 30 / 8831\n",
            "Progress: 31 / 8831\n",
            "Progress: 32 / 8831\n",
            "Progress: 33 / 8831\n",
            "Progress: 34 / 8831\n",
            "Progress: 35 / 8831\n",
            "Progress: 36 / 8831\n",
            "Progress: 37 / 8831\n",
            "Progress: 38 / 8831\n",
            "Progress: 39 / 8831\n",
            "Progress: 40 / 8831\n",
            "Progress: 41 / 8831\n",
            "Progress: 42 / 8831\n",
            "Progress: 43 / 8831\n",
            "Progress: 44 / 8831\n",
            "Progress: 45 / 8831\n",
            "Progress: 46 / 8831\n",
            "Progress: 47 / 8831\n",
            "Progress: 48 / 8831\n",
            "Progress: 49 / 8831\n",
            "Progress: 50 / 8831\n",
            "Progress: 51 / 8831\n",
            "Progress: 52 / 8831\n",
            "Progress: 53 / 8831\n",
            "Progress: 54 / 8831\n",
            "Progress: 55 / 8831\n",
            "Progress: 56 / 8831\n",
            "Progress: 57 / 8831\n",
            "Progress: 58 / 8831\n",
            "Progress: 59 / 8831\n",
            "Progress: 60 / 8831\n",
            "Progress: 61 / 8831\n",
            "Progress: 62 / 8831\n",
            "Progress: 63 / 8831\n",
            "Progress: 64 / 8831\n",
            "Progress: 65 / 8831\n",
            "Progress: 66 / 8831\n",
            "Progress: 67 / 8831\n",
            "Progress: 68 / 8831\n",
            "Progress: 69 / 8831\n",
            "Progress: 70 / 8831\n",
            "{'verified': False, 'distance': 0.5212790598142047, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 24, 'y': 29, 'w': 144, 'h': 161, 'left_eye': (111, 90), 'right_eye': (47, 95)}, 'img2': {'x': 56, 'y': 109, 'w': 218, 'h': 282, 'left_eye': (203, 218), 'right_eye': (109, 244)}}, 'time': 1.7}\n",
            "{'verified': False, 'distance': 0.4490188999028599, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 45, 'y': 61, 'w': 148, 'h': 202, 'left_eye': (156, 143), 'right_eye': (89, 142)}, 'img2': {'x': 55, 'y': 102, 'w': 194, 'h': 260, 'left_eye': (188, 183), 'right_eye': (103, 193)}}, 'time': 1.81}\n",
            "Progress: 71 / 8831\n",
            "Progress: 72 / 8831\n",
            "Progress: 73 / 8831\n",
            "{'verified': False, 'distance': 0.4164046664511679, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 54, 'y': 81, 'w': 137, 'h': 249, 'left_eye': (79, 183), 'right_eye': (75, 174)}, 'img2': {'x': 0, 'y': 62, 'w': 96, 'h': 117, 'left_eye': (27, 121), 'right_eye': (24, 121)}}, 'time': 0.83}\n",
            "Progress: 74 / 8831\n",
            "Progress: 75 / 8831\n",
            "Progress: 76 / 8831\n",
            "{'verified': False, 'distance': 0.770950360388688, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 54, 'y': 81, 'w': 137, 'h': 249, 'left_eye': (79, 183), 'right_eye': (75, 174)}, 'img2': {'x': 30, 'y': 36, 'w': 79, 'h': 87, 'left_eye': (97, 81), 'right_eye': (65, 78)}}, 'time': 0.62}\n",
            "Progress: 77 / 8831\n",
            "Progress: 78 / 8831\n",
            "Progress: 79 / 8831\n",
            "Progress: 80 / 8831\n",
            "Progress: 81 / 8831\n",
            "Progress: 82 / 8831\n",
            "Progress: 83 / 8831\n",
            "Progress: 84 / 8831\n",
            "Progress: 85 / 8831\n",
            "Progress: 86 / 8831\n",
            "Progress: 87 / 8831\n",
            "Progress: 88 / 8831\n",
            "Progress: 89 / 8831\n",
            "Progress: 90 / 8831\n",
            "Progress: 91 / 8831\n",
            "Progress: 92 / 8831\n",
            "Progress: 93 / 8831\n",
            "Progress: 94 / 8831\n",
            "Progress: 95 / 8831\n",
            "Progress: 96 / 8831\n",
            "Progress: 97 / 8831\n",
            "Progress: 98 / 8831\n",
            "Progress: 99 / 8831\n",
            "Progress: 100 / 8831\n",
            "Progress: 101 / 8831\n",
            "Progress: 102 / 8831\n",
            "Progress: 103 / 8831\n",
            "Progress: 104 / 8831\n",
            "Progress: 105 / 8831\n",
            "Progress: 106 / 8831\n",
            "Progress: 107 / 8831\n",
            "Progress: 108 / 8831\n",
            "Progress: 109 / 8831\n",
            "{'verified': False, 'distance': 0.5276925575092043, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 24, 'y': 29, 'w': 144, 'h': 161, 'left_eye': (111, 90), 'right_eye': (47, 95)}, 'img2': {'x': 35, 'y': 25, 'w': 99, 'h': 113, 'left_eye': (117, 88), 'right_eye': (79, 84)}}, 'time': 1.58}\n",
            "{'verified': False, 'distance': 0.425689649498875, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 74, 'y': 113, 'w': 222, 'h': 257, 'left_eye': (219, 190), 'right_eye': (125, 241)}, 'img2': {'x': 40, 'y': 56, 'w': 133, 'h': 188, 'left_eye': (99, 140), 'right_eye': (57, 130)}}, 'time': 0.92}\n",
            "{'verified': False, 'distance': 0.42888736566231545, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 21, 'y': 27, 'w': 77, 'h': 121, 'left_eye': (43, 74), 'right_eye': (41, 68)}, 'img2': {'x': 178, 'y': 36, 'w': 44, 'h': 140, 'left_eye': (245, 94), 'right_eye': (207, 86)}}, 'time': 1.12}\n",
            "{'verified': False, 'distance': 0.6351418316674209, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 210, 'w': 202, 'h': 281, 'left_eye': (31, 303), 'right_eye': (-52, 326)}, 'img2': {'x': 74, 'y': 92, 'w': 212, 'h': 263, 'left_eye': (255, 202), 'right_eye': (162, 182)}}, 'time': 1.64}\n",
            "{'verified': False, 'distance': 0.7893850355293098, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 21, 'y': 27, 'w': 77, 'h': 121, 'left_eye': (43, 74), 'right_eye': (41, 68)}, 'img2': {'x': 31, 'y': 46, 'w': 90, 'h': 109, 'left_eye': (106, 101), 'right_eye': (73, 80)}}, 'time': 0.8}\n",
            "{'verified': False, 'distance': 0.503153087992397, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 88, 'y': 147, 'w': 336, 'h': 425, 'left_eye': (255, 301), 'right_eye': (127, 324)}, 'img2': {'x': 78, 'y': 137, 'w': 238, 'h': 292, 'left_eye': (231, 239), 'right_eye': (121, 235)}}, 'time': 0.79}\n",
            "{'verified': False, 'distance': 0.5355674549203713, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 24, 'y': 29, 'w': 144, 'h': 161, 'left_eye': (111, 90), 'right_eye': (47, 95)}, 'img2': {'x': 49, 'y': 68, 'w': 152, 'h': 178, 'left_eye': (175, 132), 'right_eye': (109, 121)}}, 'time': 1.61}\n",
            "{'verified': True, 'distance': 0.210294911609062, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 26, 'y': 42, 'w': 94, 'h': 126, 'left_eye': (91, 92), 'right_eye': (48, 97)}, 'img2': {'x': 100, 'y': 128, 'w': 295, 'h': 373, 'left_eye': (340, 255), 'right_eye': (201, 264)}}, 'time': 0.69}\n",
            "{'verified': False, 'distance': 0.545211992747862, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 69, 'y': 109, 'w': 208, 'h': 254, 'left_eye': (229, 216), 'right_eye': (130, 206)}, 'img2': {'x': 52, 'y': 59, 'w': 159, 'h': 196, 'left_eye': (159, 131), 'right_eye': (86, 126)}}, 'time': 0.64}\n",
            "{'verified': False, 'distance': 0.3818517361957603, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 21, 'y': 27, 'w': 77, 'h': 121, 'left_eye': (43, 74), 'right_eye': (41, 68)}, 'img2': {'x': 113, 'y': 0, 'w': 40, 'h': 102, 'left_eye': (176, 20), 'right_eye': (145, 12)}}, 'time': 1.02}\n",
            "Progress: 110 / 8831\n",
            "Progress: 111 / 8831\n",
            "Progress: 112 / 8831\n",
            "Progress: 113 / 8831\n",
            "Progress: 114 / 8831\n",
            "Progress: 115 / 8831\n",
            "Progress: 116 / 8831\n",
            "Progress: 117 / 8831\n",
            "Progress: 118 / 8831\n",
            "Progress: 119 / 8831\n",
            "Progress: 120 / 8831\n",
            "{'verified': False, 'distance': 0.7346130909922705, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 35, 'y': 25, 'w': 99, 'h': 113, 'left_eye': (117, 88), 'right_eye': (79, 84)}, 'img2': {'x': 37, 'y': 71, 'w': 106, 'h': 133, 'left_eye': (133, 123), 'right_eye': (90, 128)}}, 'time': 0.57}\n",
            "Progress: 121 / 8831\n",
            "Progress: 122 / 8831\n",
            "Progress: 123 / 8831\n",
            "{'verified': False, 'distance': 0.40658551434076384, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 55, 'y': 102, 'w': 194, 'h': 260, 'left_eye': (188, 183), 'right_eye': (103, 193)}, 'img2': {'x': 60, 'y': 106, 'w': 198, 'h': 265, 'left_eye': (210, 210), 'right_eye': (118, 206)}}, 'time': 0.98}\n",
            "Progress: 124 / 8831\n",
            "Progress: 125 / 8831\n",
            "Progress: 126 / 8831\n",
            "Progress: 127 / 8831\n",
            "Progress: 128 / 8831\n",
            "Progress: 129 / 8831\n",
            "Progress: 130 / 8831\n",
            "Progress: 131 / 8831\n",
            "Progress: 132 / 8831\n",
            "Progress: 133 / 8831\n",
            "Progress: 134 / 8831\n",
            "Progress: 135 / 8831\n",
            "Progress: 136 / 8831\n",
            "Progress: 137 / 8831\n",
            "Progress: 138 / 8831\n",
            "Progress: 139 / 8831\n",
            "Progress: 140 / 8831\n",
            "Progress: 141 / 8831\n",
            "Progress: 142 / 8831\n",
            "Progress: 143 / 8831\n",
            "Progress: 144 / 8831\n",
            "Progress: 145 / 8831\n",
            "Progress: 146 / 8831\n",
            "Progress: 147 / 8831\n",
            "Progress: 148 / 8831\n",
            "Progress: 149 / 8831\n",
            "Progress: 150 / 8831\n",
            "Progress: 151 / 8831\n",
            "Progress: 152 / 8831\n",
            "{'verified': False, 'distance': 0.6707864719919916, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 30, 'y': 36, 'w': 79, 'h': 87, 'left_eye': (97, 81), 'right_eye': (65, 78)}, 'img2': {'x': 57, 'y': 58, 'w': 160, 'h': 163, 'left_eye': (175, 174), 'right_eye': (164, 138)}}, 'time': 0.57}\n",
            "Progress: 153 / 8831\n",
            "{'verified': False, 'distance': 0.5516956819143217, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 57, 'y': 58, 'w': 160, 'h': 163, 'left_eye': (175, 174), 'right_eye': (164, 138)}, 'img2': {'x': 100, 'y': 129, 'w': 261, 'h': 346, 'left_eye': (183, 286), 'right_eye': (132, 288)}}, 'time': 0.68}\n",
            "{'verified': False, 'distance': 0.6556609366130778, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 30, 'y': 36, 'w': 79, 'h': 87, 'left_eye': (97, 81), 'right_eye': (65, 78)}, 'img2': {'x': 117, 'y': 148, 'w': 319, 'h': 396, 'left_eye': (230, 334), 'right_eye': (183, 356)}}, 'time': 0.67}\n",
            "{'verified': False, 'distance': 0.3683708867348562, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 37, 'y': 71, 'w': 106, 'h': 133, 'left_eye': (133, 123), 'right_eye': (90, 128)}, 'img2': {'x': 76, 'y': 122, 'w': 179, 'h': 266, 'left_eye': (233, 212), 'right_eye': (202, 214)}}, 'time': 0.65}\n",
            "{'verified': False, 'distance': 0.5728391297557589, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 54, 'y': 81, 'w': 137, 'h': 249, 'left_eye': (79, 183), 'right_eye': (75, 174)}, 'img2': {'x': 50, 'y': 98, 'w': 201, 'h': 300, 'left_eye': (109, 234), 'right_eye': (81, 228)}}, 'time': 0.7}\n",
            "Progress: 154 / 8831\n",
            "{'verified': False, 'distance': 0.7496845489191375, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 30, 'y': 36, 'w': 79, 'h': 87, 'left_eye': (97, 81), 'right_eye': (65, 78)}, 'img2': {'x': 108, 'y': 198, 'w': 367, 'h': 476, 'left_eye': (195, 419), 'right_eye': (159, 450)}}, 'time': 0.81}\n",
            "{'verified': False, 'distance': 0.35922743648894606, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 50, 'y': 98, 'w': 201, 'h': 300, 'left_eye': (109, 234), 'right_eye': (81, 228)}, 'img2': {'x': 172, 'y': 308, 'w': 369, 'h': 533, 'left_eye': (493, 518), 'right_eye': (436, 518)}}, 'time': 1.0}\n",
            "{'verified': False, 'distance': 0.6516875674432195, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 54, 'y': 81, 'w': 137, 'h': 249, 'left_eye': (79, 183), 'right_eye': (75, 174)}, 'img2': {'x': 57, 'y': 129, 'w': 236, 'h': 392, 'left_eye': (253, 297), 'right_eye': (231, 289)}}, 'time': 0.79}\n",
            "Progress: 155 / 8831\n",
            "{'verified': False, 'distance': 0.5841925548680613, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 100, 'y': 128, 'w': 295, 'h': 373, 'left_eye': (340, 255), 'right_eye': (201, 264)}, 'img2': {'x': 106, 'y': 184, 'w': 300, 'h': 441, 'left_eye': (368, 367), 'right_eye': (266, 366)}}, 'time': 1.15}\n",
            "{'verified': False, 'distance': 0.5298345176750958, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 74, 'y': 92, 'w': 212, 'h': 263, 'left_eye': (255, 202), 'right_eye': (162, 182)}, 'img2': {'x': 112, 'y': 244, 'w': 323, 'h': 525, 'left_eye': (388, 438), 'right_eye': (361, 455)}}, 'time': 1.85}\n",
            "{'verified': False, 'distance': 0.7758655243275504, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 59, 'y': 101, 'w': 151, 'h': 205, 'left_eye': (126, 194), 'right_eye': (77, 193)}, 'img2': {'x': 99, 'y': 201, 'w': 284, 'h': 388, 'left_eye': (239, 379), 'right_eye': (142, 346)}}, 'time': 1.91}\n",
            "{'verified': False, 'distance': 0.34480202194720955, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 83, 'y': 97, 'w': 193, 'h': 298, 'left_eye': (249, 226), 'right_eye': (220, 222)}, 'img2': {'x': 118, 'y': 329, 'w': 225, 'h': 360, 'left_eye': (176, 470), 'right_eye': (162, 466)}}, 'time': 1.84}\n",
            "Progress: 156 / 8831\n",
            "Progress: 157 / 8831\n",
            "Progress: 158 / 8831\n",
            "Progress: 159 / 8831\n",
            "Progress: 160 / 8831\n",
            "Progress: 161 / 8831\n",
            "Progress: 162 / 8831\n",
            "Progress: 163 / 8831\n",
            "Progress: 164 / 8831\n",
            "Progress: 165 / 8831\n",
            "Progress: 166 / 8831\n",
            "Progress: 167 / 8831\n",
            "Progress: 168 / 8831\n",
            "Progress: 169 / 8831\n",
            "Progress: 170 / 8831\n",
            "Progress: 171 / 8831\n",
            "Progress: 172 / 8831\n",
            "Progress: 173 / 8831\n",
            "Progress: 174 / 8831\n",
            "Progress: 175 / 8831\n",
            "Progress: 176 / 8831\n",
            "Progress: 177 / 8831\n",
            "Progress: 178 / 8831\n",
            "Progress: 179 / 8831\n",
            "Progress: 180 / 8831\n",
            "Progress: 181 / 8831\n",
            "Progress: 182 / 8831\n",
            "Progress: 183 / 8831\n",
            "Progress: 184 / 8831\n",
            "Progress: 185 / 8831\n",
            "Progress: 186 / 8831\n",
            "Progress: 187 / 8831\n",
            "Progress: 188 / 8831\n",
            "Progress: 189 / 8831\n",
            "Progress: 190 / 8831\n",
            "Progress: 191 / 8831\n",
            "{'verified': False, 'distance': 0.3595914267851922, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 23, 'y': 33, 'w': 85, 'h': 105, 'left_eye': (76, 73), 'right_eye': (39, 69)}, 'img2': {'x': 64, 'y': 111, 'w': 195, 'h': 254, 'left_eye': (158, 227), 'right_eye': (90, 206)}}, 'time': 0.63}\n",
            "{'verified': False, 'distance': 0.5244022533714517, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 37, 'y': 71, 'w': 106, 'h': 133, 'left_eye': (133, 123), 'right_eye': (90, 128)}, 'img2': {'x': 83, 'y': 90, 'w': 194, 'h': 233, 'left_eye': (254, 192), 'right_eye': (206, 188)}}, 'time': 0.9}\n",
            "{'verified': False, 'distance': 0.5015537323852557, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 40, 'y': 56, 'w': 133, 'h': 188, 'left_eye': (99, 140), 'right_eye': (57, 130)}, 'img2': {'x': 49, 'y': 85, 'w': 162, 'h': 198, 'left_eye': (181, 154), 'right_eye': (102, 156)}}, 'time': 0.63}\n",
            "{'verified': False, 'distance': 0.5499279790988858, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 100, 'y': 129, 'w': 261, 'h': 346, 'left_eye': (183, 286), 'right_eye': (132, 288)}, 'img2': {'x': 53, 'y': 92, 'w': 149, 'h': 185, 'left_eye': (158, 171), 'right_eye': (90, 168)}}, 'time': 1.35}\n",
            "{'verified': False, 'distance': 0.4184477744294125, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 83, 'y': 97, 'w': 193, 'h': 298, 'left_eye': (249, 226), 'right_eye': (220, 222)}, 'img2': {'x': 54, 'y': 118, 'w': 195, 'h': 293, 'left_eye': (214, 219), 'right_eye': (207, 232)}}, 'time': 1.44}\n",
            "Progress: 192 / 8831\n",
            "{'verified': False, 'distance': 0.6004474760206362, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 83, 'y': 97, 'w': 193, 'h': 298, 'left_eye': (249, 226), 'right_eye': (220, 222)}, 'img2': {'x': 26, 'y': 50, 'w': 294, 'h': 281, 'left_eye': (144, 197), 'right_eye': (84, 226)}}, 'time': 0.74}\n",
            "{'verified': True, 'distance': 0.1776757223801324, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 44, 'y': 68, 'w': 131, 'h': 160, 'left_eye': (152, 145), 'right_eye': (101, 122)}, 'img2': {'x': 85, 'y': 126, 'w': 270, 'h': 391, 'left_eye': (273, 298), 'right_eye': (151, 291)}}, 'time': 1.61}\n",
            "{'verified': False, 'distance': 0.4082495181474831, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 49, 'y': 68, 'w': 152, 'h': 178, 'left_eye': (175, 132), 'right_eye': (109, 121)}, 'img2': {'x': 434, 'y': 333, 'w': 254, 'h': 281, 'left_eye': (543, 468), 'right_eye': (477, 488)}}, 'time': 1.63}\n",
            "{'verified': False, 'distance': 0.3162345558055436, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 54, 'y': 81, 'w': 137, 'h': 249, 'left_eye': (79, 183), 'right_eye': (75, 174)}, 'img2': {'x': 0, 'y': 298, 'w': 592, 'h': 950, 'left_eye': (457, 672), 'right_eye': (486, 628)}}, 'time': 1.76}\n",
            "{'verified': False, 'distance': 0.30678826732207753, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 83, 'y': 97, 'w': 193, 'h': 298, 'left_eye': (249, 226), 'right_eye': (220, 222)}, 'img2': {'x': 64, 'y': 210, 'w': 494, 'h': 562, 'left_eye': (241, 493), 'right_eye': (211, 563)}}, 'time': 1.09}\n",
            "Progress: 193 / 8831\n",
            "{'verified': False, 'distance': 0.3303336573953186, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 64, 'y': 111, 'w': 195, 'h': 254, 'left_eye': (158, 227), 'right_eye': (90, 206)}, 'img2': {'x': 56, 'y': 93, 'w': 190, 'h': 223, 'left_eye': (160, 191), 'right_eye': (81, 184)}}, 'time': 0.72}\n",
            "{'verified': True, 'distance': 0.24769239058265957, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 49, 'y': 85, 'w': 162, 'h': 198, 'left_eye': (181, 154), 'right_eye': (102, 156)}, 'img2': {'x': 54, 'y': 74, 'w': 157, 'h': 182, 'left_eye': (170, 150), 'right_eye': (102, 126)}}, 'time': 0.87}\n",
            "{'verified': False, 'distance': 0.43203255852180733, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 32, 'y': 47, 'w': 92, 'h': 118, 'left_eye': (104, 108), 'right_eye': (67, 86)}, 'img2': {'x': 45, 'y': 83, 'w': 137, 'h': 169, 'left_eye': (148, 150), 'right_eye': (81, 154)}}, 'time': 1.09}\n",
            "{'verified': False, 'distance': 0.7589256454821912, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 118, 'y': 329, 'w': 225, 'h': 360, 'left_eye': (176, 470), 'right_eye': (162, 466)}, 'img2': {'x': 58, 'y': 87, 'w': 128, 'h': 180, 'left_eye': (155, 140), 'right_eye': (109, 152)}}, 'time': 1.69}\n",
            "{'verified': False, 'distance': 0.7004483356908913, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 83, 'y': 97, 'w': 193, 'h': 298, 'left_eye': (249, 226), 'right_eye': (220, 222)}, 'img2': {'x': 66, 'y': 84, 'w': 211, 'h': 293, 'left_eye': (116, 203), 'right_eye': (93, 187)}}, 'time': 0.69}\n",
            "{'verified': False, 'distance': 0.8001356206247354, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 64, 'y': 210, 'w': 494, 'h': 562, 'left_eye': (241, 493), 'right_eye': (211, 563)}, 'img2': {'x': 0, 'y': 150, 'w': 673, 'h': 725, 'left_eye': (315, 354), 'right_eye': (167, 561)}}, 'time': 1.02}\n",
            "{'verified': True, 'distance': 0.13064165977922404, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 53, 'y': 92, 'w': 149, 'h': 185, 'left_eye': (158, 171), 'right_eye': (90, 168)}, 'img2': {'x': 66, 'y': 72, 'w': 166, 'h': 206, 'left_eye': (209, 160), 'right_eye': (134, 168)}}, 'time': 1.4}\n",
            "{'verified': False, 'distance': 0.59350471502485, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 0, 'y': 298, 'w': 592, 'h': 950, 'left_eye': (457, 672), 'right_eye': (486, 628)}, 'img2': {'x': 29, 'y': 109, 'w': 200, 'h': 381, 'left_eye': (107, 262), 'right_eye': (63, 265)}}, 'time': 1.8}\n",
            "Progress: 194 / 8831\n",
            "Progress: 195 / 8831\n",
            "Progress: 196 / 8831\n",
            "Progress: 197 / 8831\n",
            "Progress: 198 / 8831\n",
            "Progress: 199 / 8831\n",
            "Progress: 200 / 8831\n",
            "{'verified': False, 'distance': 0.4631557300953666, 'threshold': 0.3, 'model': 'Facenet512', 'detector_backend': 'retinaface', 'similarity_metric': 'cosine', 'facial_areas': {'img1': {'x': 434, 'y': 333, 'w': 254, 'h': 281, 'left_eye': (543, 468), 'right_eye': (477, 488)}, 'img2': {'x': 50, 'y': 9, 'w': 119, 'h': 138, 'left_eye': (143, 67), 'right_eye': (87, 58)}}, 'time': 1.31}\n",
            "Progress: 201 / 8831\n",
            "Progress: 202 / 8831\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-12a5fefe58dd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mimg_path\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetinaFace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetect_faces\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mwrite_processed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/retinaface/RetinaFace.py\u001b[0m in \u001b[0;36mdetect_faces\u001b[0;34m(img_path, threshold, model, allow_upscaling)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \"\"\"\n\u001b[1;32m     90\u001b[0m     \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# ---------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/retinaface/commons/preprocess.py\u001b[0m in \u001b[0;36mget_image\u001b[0;34m(img_uri)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;31m# pylint: disable=no-member\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_uri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "drawImageWithPlot(img_path, resp, False, show_img=True)"
      ],
      "metadata": {
        "id": "_LDsgRA3ASko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "u8zOBCQmT8fk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing Area"
      ],
      "metadata": {
        "id": "KctpCGjKszf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# face_objs = DeepFace.extract_faces(\n",
        "#   img_path = \"test.jpg\",\n",
        "#   detector_backend = \"dlib\",\n",
        "#   align = True,\n",
        "# )\n",
        "# face_objs"
      ],
      "metadata": {
        "id": "TnP5NcoiAPXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# img = cv2.imread('test.jpg')\n",
        "# plt.imshow(img[:, :, ::-1])"
      ],
      "metadata": {
        "id": "9kFch5DqS8fe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# type(face_objs)"
      ],
      "metadata": {
        "id": "iXfuRpjcTU0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# analyze one image\n",
        "# img_path = \"test2.jpg\"\n",
        "# resp = RetinaFace.detect_faces(img_path)"
      ],
      "metadata": {
        "id": "qv46pjbQWiXp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# output image\n",
        "# drawImageWithPlot(img_path, resp, True)"
      ],
      "metadata": {
        "id": "udn1u22wWqCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# result = DeepFace.find(img_path=\"saved_crop_0.jpg\", db_path=\"find_image\", model_name=\"VGG-Face\", enforce_detection=False, silent=True)"
      ],
      "metadata": {
        "id": "N9m3vzCC7Jzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(result)"
      ],
      "metadata": {
        "id": "95L6tjug_cWf"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}